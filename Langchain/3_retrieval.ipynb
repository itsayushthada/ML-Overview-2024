{"cells":[{"cell_type":"markdown","id":"0689733d","metadata":{"id":"0689733d"},"source":["# Retrieval\n","\n","The Retrieval is the centerpiece of our retrieval augmented generation (RAG) system.\n"]},{"cell_type":"markdown","id":"ed2569c6","metadata":{"id":"ed2569c6"},"source":["## Vectorstore retrieval\n"]},{"cell_type":"code","execution_count":1,"id":"t6t3dH7w5WXs","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t6t3dH7w5WXs","outputId":"f6624953-9756-4191-8cf7-e2661b33e832"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: langchain in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (0.0.311)Note: you may need to restart the kernel to use updated packages.\n","\n","Requirement already satisfied: chromadb in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (0.3.26)\n","Requirement already satisfied: pypdf in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (4.2.0)\n","Requirement already satisfied: langchain-community in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (0.0.20)\n","Requirement already satisfied: PyYAML>=5.3 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from langchain) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from langchain) (2.0.29)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from langchain) (3.9.5)\n","Requirement already satisfied: anyio<4.0 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from langchain) (3.7.1)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from langchain) (4.0.3)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from langchain) (0.6.5)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from langchain) (1.33)\n","Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from langchain) (0.0.87)\n","Requirement already satisfied: numpy<2,>=1 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from langchain) (1.26.4)\n","Requirement already satisfied: pydantic<3,>=1 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from langchain) (1.10.13)\n","Requirement already satisfied: requests<3,>=2 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from langchain) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from langchain) (8.2.3)\n","Requirement already satisfied: pandas>=1.3 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from chromadb) (2.2.2)\n","Requirement already satisfied: hnswlib>=0.7 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from chromadb) (0.8.0)\n","Requirement already satisfied: clickhouse-connect>=0.5.7 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from chromadb) (0.7.8)\n","Requirement already satisfied: duckdb>=0.7.1 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from chromadb) (0.10.2)\n","Requirement already satisfied: fastapi>=0.85.1 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from chromadb) (0.103.2)\n","Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.23.2)\n","Requirement already satisfied: posthog>=2.4.0 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from chromadb) (3.5.0)\n","Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from chromadb) (4.11.0)\n","Requirement already satisfied: pulsar-client>=3.1.0 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from chromadb) (3.5.0)\n","Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from chromadb) (1.17.3)\n","Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from chromadb) (0.19.1)\n","Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from chromadb) (4.66.4)\n","Requirement already satisfied: overrides>=7.3.1 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from chromadb) (7.7.0)\n","Requirement already satisfied: langchain-core<0.2,>=0.1.21 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from langchain-community) (0.1.23)\n","Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n","Requirement already satisfied: idna>=2.8 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from anyio<4.0->langchain) (3.7)\n","Requirement already satisfied: sniffio>=1.1 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from anyio<4.0->langchain) (1.3.1)\n","Requirement already satisfied: exceptiongroup in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from anyio<4.0->langchain) (1.2.0)\n","Requirement already satisfied: certifi in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb) (2024.2.2)\n","Requirement already satisfied: urllib3>=1.26 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb) (2.2.1)\n","Requirement already satisfied: pytz in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb) (2024.1)\n","Requirement already satisfied: zstandard in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb) (0.22.0)\n","Requirement already satisfied: lz4 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from clickhouse-connect>=0.5.7->chromadb) (4.3.3)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n","Requirement already satisfied: starlette<0.28.0,>=0.27.0 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from fastapi>=0.85.1->chromadb) (0.27.0)\n","Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n","Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from langchain-core<0.2,>=0.1.21->langchain-community) (23.2)\n","Requirement already satisfied: coloredlogs in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n","Requirement already satisfied: flatbuffers in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n","Requirement already satisfied: protobuf in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.26.1)\n","Requirement already satisfied: sympy in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from pandas>=1.3->chromadb) (2.9.0)\n","Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from pandas>=1.3->chromadb) (2024.1)\n","Requirement already satisfied: six>=1.5 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n","Requirement already satisfied: monotonic>=1.5 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n","Requirement already satisfied: backoff>=1.10.0 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.23.0)\n","Requirement already satisfied: colorama in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from tqdm>=4.65.0->chromadb) (0.4.6)\n","Requirement already satisfied: click>=7.0 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (8.1.7)\n","Requirement already satisfied: h11>=0.8 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n","Requirement already satisfied: httptools>=0.5.0 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n","Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n","Requirement already satisfied: watchfiles>=0.13 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n","Requirement already satisfied: websockets>=10.4 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n","Requirement already satisfied: filelock in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.14.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.3.1)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n","Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n","Requirement already satisfied: mpmath>=0.19 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n","Requirement already satisfied: pyreadline3 in c:\\users\\admin\\miniconda3\\envs\\py310\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.4.1)\n"]}],"source":["%pip install langchain chromadb pypdf langchain-community"]},{"cell_type":"markdown","id":"c2d552e1","metadata":{"id":"c2d552e1"},"source":["### Similarity Search"]},{"cell_type":"code","execution_count":2,"id":"57677228","metadata":{},"outputs":[],"source":["from langchain.vectorstores import Chroma\n","from utils import SaladOllamaEmbeddings"]},{"cell_type":"code","execution_count":3,"id":"14c311a0","metadata":{},"outputs":[],"source":["embedding = SaladOllamaEmbeddings()\n","\n","documents = [\n","    \"The quick brown fox jumps over the lazy dog.\",\n","    \"Python is a popular programming language.\",\n","    \"Machine learning is a subset of artificial intelligence.\",\n","    \"Data science involves extracting insights from data.\"\n","]"]},{"cell_type":"code","execution_count":5,"id":"507c3d35","metadata":{},"outputs":[],"source":["chroma_db = Chroma.from_texts(documents, embedding=embedding)\n","# chroma_db.delete(chroma_db.get()[\"ids\"])"]},{"cell_type":"code","execution_count":6,"id":"1d0cf070","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["4\n"]}],"source":["print(chroma_db._collection.count())"]},{"cell_type":"code","execution_count":7,"id":"9d473965","metadata":{},"outputs":[{"data":{"text/plain":["[Document(page_content='Python is a popular programming language.'),\n"," Document(page_content='Machine learning is a subset of artificial intelligence.')]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["question  = \"python programming language ?\"\n","chroma_db.similarity_search(question, k=2)"]},{"cell_type":"markdown","id":"36997bb7","metadata":{},"source":["## Load some pdf into chromadb"]},{"cell_type":"code","execution_count":9,"id":"9f8de2ef","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Ignoring wrong pointing object 6 0 (offset 0)\n","Ignoring wrong pointing object 8 0 (offset 0)\n","Ignoring wrong pointing object 10 0 (offset 0)\n","Ignoring wrong pointing object 6 0 (offset 0)\n","Ignoring wrong pointing object 8 0 (offset 0)\n","Ignoring wrong pointing object 10 0 (offset 0)\n","Ignoring wrong pointing object 6 0 (offset 0)\n","Ignoring wrong pointing object 8 0 (offset 0)\n","Ignoring wrong pointing object 10 0 (offset 0)\n","Ignoring wrong pointing object 6 0 (offset 0)\n","Ignoring wrong pointing object 8 0 (offset 0)\n","Ignoring wrong pointing object 10 0 (offset 0)\n"]}],"source":["from langchain.document_loaders import PyPDFLoader\n","\n","# Load PDF\n","loaders = [\n","    # Duplicate documents on purpose - messy data\n","    PyPDFLoader(\"data/machine_learning_linear_reg.pdf\"),\n","    PyPDFLoader(\"data/machine_learning_linear_reg.pdf\"),\n","    PyPDFLoader(\"data/machine_learning_Decision Tree.pdf\"),\n","    PyPDFLoader(\"data/machine_learning_XGBoost.pdf\")\n","]\n","docs = []\n","for loader in loaders:\n","    docs.extend(loader.load())"]},{"cell_type":"code","execution_count":10,"id":"02f17053","metadata":{},"outputs":[{"data":{"text/plain":["[Document(page_content='                  Linear Regression  Introduc)on  Linear regression is a fundamental supervised learning algorithm used in the ﬁeld of sta5s5cs and machine learning. It is employed to establish the rela5onship between a dependent variable and one or more independent variables. The objec5ve of linear regression is to ﬁnd the best-ﬁ?ng straight line that can depict the rela5onship between the variables. This line serves as a predic5ve model for future data points.  How It Works  Linear regression works by minimizing the ver5cal distances between the observed data points and the predicted values generated by the linear approxima5on. It accomplishes this through the method of least squares, which involves minimizing the sum of the squares of the diﬀerences between the observed and predicted values. The algorithm computes the slope and intercept of the line that minimizes the overall error, thereby determining the best-ﬁt line.  Mathema)cal Intui)on  The equa5on for a simple linear regression is represented as:   \\\\[Y = \\\\beta_0 + \\\\beta_1X + \\\\varepsilon\\\\]  where: - \\\\(Y\\\\) is the dependent variable, - \\\\(X\\\\) is the independent variable, - \\\\(\\\\beta_0\\\\) is the intercept, - \\\\(\\\\beta_1\\\\) is the coeﬃcient for the independent variable, - \\\\(\\\\varepsilon\\\\) represents the error term or residual.  Limita)ons  Linear regression has certain limita5ons, including its reliance on the linearity assump5on between the dependent and independent variables. If the rela5onship between the variables is non-linear, the model may not accurately represent the data. Addi5onally, linear regression is sensi5ve to outliers, and its performance may be impacted by the presence of mul5collinearity among the independent variables.  ', metadata={'source': 'data/machine_learning_linear_reg.pdf', 'page': 0}),\n"," Document(page_content='Advantages  Despite its limita5ons, linear regression oﬀers various advantages. It provides a simple and interpretable framework for understanding the rela5onship between variables. It is computa5onally eﬃcient and well-suited for scenarios where the rela5onship between variables can be adequately captured by a linear model. Moreover, linear regression serves as a fundamental building block for more complex regression models and is widely used for predic5ve analy5cs and forecas5ng tasks.  Disadvantages  One of the signiﬁcant drawbacks of linear regression is its inability to capture complex rela5onships between variables. It may not perform well when the data exhibits non-linear paXerns. Addi5onally, it assumes that the errors follow a normal distribu5on, which might not hold true for all datasets. Moreover, the presence of outliers can signiﬁcantly impact the accuracy of the model, leading to biased results.  Understanding the intricacies and trade-oﬀs of linear regression is crucial for eﬀec5vely applying this technique in various real-world applica5ons.  ', metadata={'source': 'data/machine_learning_linear_reg.pdf', 'page': 1}),\n"," Document(page_content='                  Linear Regression  Introduc)on  Linear regression is a fundamental supervised learning algorithm used in the ﬁeld of sta5s5cs and machine learning. It is employed to establish the rela5onship between a dependent variable and one or more independent variables. The objec5ve of linear regression is to ﬁnd the best-ﬁ?ng straight line that can depict the rela5onship between the variables. This line serves as a predic5ve model for future data points.  How It Works  Linear regression works by minimizing the ver5cal distances between the observed data points and the predicted values generated by the linear approxima5on. It accomplishes this through the method of least squares, which involves minimizing the sum of the squares of the diﬀerences between the observed and predicted values. The algorithm computes the slope and intercept of the line that minimizes the overall error, thereby determining the best-ﬁt line.  Mathema)cal Intui)on  The equa5on for a simple linear regression is represented as:   \\\\[Y = \\\\beta_0 + \\\\beta_1X + \\\\varepsilon\\\\]  where: - \\\\(Y\\\\) is the dependent variable, - \\\\(X\\\\) is the independent variable, - \\\\(\\\\beta_0\\\\) is the intercept, - \\\\(\\\\beta_1\\\\) is the coeﬃcient for the independent variable, - \\\\(\\\\varepsilon\\\\) represents the error term or residual.  Limita)ons  Linear regression has certain limita5ons, including its reliance on the linearity assump5on between the dependent and independent variables. If the rela5onship between the variables is non-linear, the model may not accurately represent the data. Addi5onally, linear regression is sensi5ve to outliers, and its performance may be impacted by the presence of mul5collinearity among the independent variables.  ', metadata={'source': 'data/machine_learning_linear_reg.pdf', 'page': 0}),\n"," Document(page_content='Advantages  Despite its limita5ons, linear regression oﬀers various advantages. It provides a simple and interpretable framework for understanding the rela5onship between variables. It is computa5onally eﬃcient and well-suited for scenarios where the rela5onship between variables can be adequately captured by a linear model. Moreover, linear regression serves as a fundamental building block for more complex regression models and is widely used for predic5ve analy5cs and forecas5ng tasks.  Disadvantages  One of the signiﬁcant drawbacks of linear regression is its inability to capture complex rela5onships between variables. It may not perform well when the data exhibits non-linear paXerns. Addi5onally, it assumes that the errors follow a normal distribu5on, which might not hold true for all datasets. Moreover, the presence of outliers can signiﬁcantly impact the accuracy of the model, leading to biased results.  Understanding the intricacies and trade-oﬀs of linear regression is crucial for eﬀec5vely applying this technique in various real-world applica5ons.  ', metadata={'source': 'data/machine_learning_linear_reg.pdf', 'page': 1}),\n"," Document(page_content='                      Decision Tree   Introduc)on  Decision trees are versa-le supervised learning algorithms used for both classiﬁca-on and regression tasks in machine learning. They mimic the human decision-making process by crea-ng a model that predicts the value of a target variable based on several input features. Decision trees par--on the data into subsets based on the selected features, with each par--on represen-ng a node in the tree.  How It Works  The algorithm recursively splits the dataset into smaller subsets based on the most inﬂuen-al features at each node. It selects the feature that best separates the data points, crea-ng branches that represent diﬀerent outcomes. The process con-nues un-l the algorithm iden-ﬁes the best possible outcome, or un-l a stopping criterion is met. The resul-ng structure resembles a tree, with branches represen-ng decisions and leaves represen-ng the ﬁnal outcome or predic-on.  Mathema)cal Intui)on  The decision tree algorithm uses various metrics, such as Gini impurity or informa-on gain, to determine the op-mal feature for par--oning the data at each node. It selects the feature that maximizes the homogeneity of the target variable within each subset. The algorithm recursively applies this process to create a tree that best classiﬁes or predicts the target variable.  Limita)ons  Despite their versa-lity, decision trees can be prone to overﬁEng, especially when dealing with complex datasets. They may create overly complex trees that fail to generalize well to unseen data. Decision trees are also sensi-ve to small varia-ons in the training data and can be unstable, leading to diﬀerent results with slight changes in the input data. Addi-onally, decision trees can struggle to capture rela-onships between features that are not explicitly represented in the data.  Advantages  ', metadata={'source': 'data/machine_learning_Decision Tree.pdf', 'page': 0}),\n"," Document(page_content='Decision trees oﬀer various advantages, including their interpretability and ease of understanding. They can handle both numerical and categorical data, making them suitable for a wide range of applica-ons. Decision trees require minimal data preprocessing and can handle missing values. They are also robust to outliers and do not require feature scaling. Furthermore, decision trees can provide insights into the most cri-cal features driving the decision-making process.  Disadvantages  One of the main drawbacks of decision trees is their tendency to overﬁt the training data, leading to poor generaliza-on on unseen data. They may not capture complex rela-onships well, especially when dealing with high-dimensional data. Addi-onally, small changes in the data can lead to signiﬁcant changes in the resul-ng tree structure, making them less stable compared to other algorithms.  Understanding the nuances and trade-oﬀs associated with decision trees is crucial for eﬀec-vely applying them in various classiﬁca-on and regression tasks. ', metadata={'source': 'data/machine_learning_Decision Tree.pdf', 'page': 1}),\n"," Document(page_content='                             XGBoost   Introduc)on  XGBoost (Extreme Gradient Boos2ng) is a powerful and eﬃcient implementa2on of the gradient boos2ng algorithm. It is widely used for supervised learning tasks, including classiﬁca2on, regression, and ranking problems. XGBoost builds an ensemble of weak predic2on models, typically decision trees, to create a strong predic2ve model with high accuracy and generalizability.  How It Works  XGBoost sequen2ally builds a strong model by adding weak models that collec2vely minimize a predeﬁned loss func2on. It uses gradient boos2ng to op2mize the model by ﬁGng new models to the residuals of the previous models. XGBoost employs a combina2on of regulariza2on techniques and parallel processing to enhance model performance and reduce overﬁGng.  Mathema)cal Intui)on  XGBoost op2mizes the objec2ve func2on by compu2ng the ﬁrst and second deriva2ves of the loss func2on. It uses the Taylor series expansion to approximate the loss func2on, leading to a simpliﬁed yet eﬀec2ve op2miza2on process. XGBoost u2lizes techniques like gradient descent and tree pruning to eﬃciently handle complex datasets and achieve beKer generaliza2on. Limita)ons  Although XGBoost is known for its robustness and high performance, it can be computa2onally expensive and memory-intensive, par2cularly for large datasets. It may require ﬁne-tuning of various hyperparameters to achieve the best results, making it more complex to implement compared to simpler algorithms. Addi2onally, the interpretability of the resul2ng models may be challenging due to the complexity of the ensemble learning process. Advantages  XGBoost oﬀers several advantages, including its ability to handle complex, high-dimensional data and its robustness against overﬁGng. It provides high predic2ve accuracy and generalizability, making it suitable for a wide range of real-world applica2ons. XGBoost is highly scalable and can eﬃciently handle large datasets. It also oﬀers ﬂexibility in terms of ', metadata={'source': 'data/machine_learning_XGBoost.pdf', 'page': 0}),\n"," Document(page_content='customiza2on and hyperparameter tuning, allowing users to op2mize model performance based on speciﬁc requirements. Disadvantages  Despite its advantages, XGBoost has certain limita2ons, such as the increased computa2onal complexity and longer training 2mes compared to simpler algorithms. It may require signiﬁcant computa2onal resources, limi2ng its applicability in resource-constrained environments. The black-box nature of the model can also hinder interpretability, making it challenging to understand the decision-making process for complex predic2ons. ', metadata={'source': 'data/machine_learning_XGBoost.pdf', 'page': 1})]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["docs"]},{"cell_type":"code","execution_count":13,"id":"c829a5c7","metadata":{},"outputs":[{"data":{"text/plain":["12"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["\n","# Split\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size = 1500,\n","    chunk_overlap = 150\n",")\n","splits = text_splitter.split_documents(docs)\n","len(splits)\n"]},{"cell_type":"code","execution_count":14,"id":"1146359a","metadata":{},"outputs":[],"source":["# vectordb.delete(ids=vectordb.get()[\"ids\"])\n","\n","vectordb = Chroma.from_documents(\n","    documents=splits,\n","    embedding=embedding,\n","    persist_directory = \"retrieval_db\"\n",")"]},{"cell_type":"code","execution_count":130,"id":"3659e0f7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3659e0f7","outputId":"9d1983d6-bec5-4501-b976-8bf76705b098","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["24\n"]}],"source":["print(vectordb._collection.count())"]},{"cell_type":"code","execution_count":48,"id":"9bb2c0a9","metadata":{"id":"9bb2c0a9","tags":[]},"outputs":[],"source":["question = \"What are the advantages of Linear Regression \"\n","docs_ss = vectordb.similarity_search(question,k=3)"]},{"cell_type":"code","execution_count":49,"id":"1f225ffc","metadata":{},"outputs":[{"data":{"text/plain":["[Document(page_content='including its reliance on the linearity assump5on between the dependent and independent variables. If the rela5onship between the variables is non-linear, the model may not accurately represent the data. Addi5onally, linear regression is sensi5ve to outliers, and its performance may be impacted by the presence of mul5collinearity among the independent variables.', metadata={'page': 0, 'source': 'data/machine_learning_linear_reg.pdf'}),\n"," Document(page_content='including its reliance on the linearity assump5on between the dependent and independent variables. If the rela5onship between the variables is non-linear, the model may not accurately represent the data. Addi5onally, linear regression is sensi5ve to outliers, and its performance may be impacted by the presence of mul5collinearity among the independent variables.', metadata={'page': 0, 'source': 'data/machine_learning_linear_reg.pdf'}),\n"," Document(page_content='Advantages  Despite its limita5ons, linear regression oﬀers various advantages. It provides a simple and interpretable framework for understanding the rela5onship between variables. It is computa5onally eﬃcient and well-suited for scenarios where the rela5onship between variables can be adequately captured by a linear model. Moreover, linear regression serves as a fundamental building block for more complex regression models and is widely used for predic5ve analy5cs and forecas5ng tasks.  Disadvantages  One of the signiﬁcant drawbacks of linear regression is its inability to capture complex rela5onships between variables. It may not perform well when the data exhibits non-linear paXerns. Addi5onally, it assumes that the errors follow a normal distribu5on, which might not hold true for all datasets. Moreover, the presence of outliers can signiﬁcantly impact the accuracy of the model, leading to biased results.  Understanding the intricacies and trade-oﬀs of linear regression is crucial for eﬀec5vely applying this technique in various real-world applica5ons.', metadata={'page': 1, 'source': 'data/machine_learning_linear_reg.pdf'})]"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["docs_ss"]},{"cell_type":"code","execution_count":50,"id":"f07f8793","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"f07f8793","outputId":"393785a0-62ac-4046-a277-c5e264ebfcd5","tags":[]},"outputs":[{"data":{"text/plain":["'including its reliance on the linearity assump5on between the dependent and independent variables. If the rela5onship between the variables is non-linear, the model may not accurately represent the da'"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["docs_ss[0].page_content[:200]"]},{"cell_type":"code","execution_count":51,"id":"e9f7e165","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"e9f7e165","outputId":"d140c643-92ec-4fd0-89fe-960375d3375e","tags":[]},"outputs":[{"data":{"text/plain":["'including its reliance on the linearity assump5on between the dependent and independent variables. If the rela5onship between the variables is non-linear, the model may not accurately represent the da'"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["docs_ss[1].page_content[:200]"]},{"cell_type":"markdown","id":"b46e8ac7","metadata":{},"source":["### Addressing Diversity: Maximum marginal relevance\n","\n","Last class we introduced one problem: how to enforce diversity in the search results.\n","\n","`Maximum marginal relevance` strives to achieve both relevance to the query *and diversity* among the results."]},{"cell_type":"markdown","id":"4c4ca1b6","metadata":{"id":"4c4ca1b6"},"source":["Note the difference in results with `MMR`."]},{"cell_type":"code","execution_count":52,"id":"222234c5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"222234c5","outputId":"a393dd64-3ec7-4592-da38-b86cc53dfcfe","tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["Number of requested results 20 is greater than number of elements in index 12, updating n_results = 12\n"]}],"source":["docs_mmr = vectordb.max_marginal_relevance_search(question,k=3)"]},{"cell_type":"code","execution_count":53,"id":"fe72a1a6","metadata":{},"outputs":[{"data":{"text/plain":["[Document(page_content='including its reliance on the linearity assump5on between the dependent and independent variables. If the rela5onship between the variables is non-linear, the model may not accurately represent the data. Addi5onally, linear regression is sensi5ve to outliers, and its performance may be impacted by the presence of mul5collinearity among the independent variables.', metadata={'page': 0, 'source': 'data/machine_learning_linear_reg.pdf'}),\n"," Document(page_content='their versa-lity, decision trees can be prone to overﬁEng, especially when dealing with complex datasets. They may create overly complex trees that fail to generalize well to unseen data. Decision trees are also sensi-ve to small varia-ons in the training data and can be unstable, leading to diﬀerent results with slight changes in the input data. Addi-onally, decision trees can struggle to capture rela-onships between features that are not explicitly represented in the data.  Advantages', metadata={'page': 0, 'source': 'data/machine_learning_Decision Tree.pdf'}),\n"," Document(page_content='Decision Tree   Introduc)on  Decision trees are versa-le supervised learning algorithms used for both classiﬁca-on and regression tasks in machine learning. They mimic the human decision-making process by crea-ng a model that predicts the value of a target variable based on several input features. Decision trees par--on the data into subsets based on the selected features, with each par--on represen-ng a node in the tree.  How It Works  The algorithm recursively splits the dataset into smaller subsets based on the most inﬂuen-al features at each node. It selects the feature that best separates the data points, crea-ng branches that represent diﬀerent outcomes. The process con-nues un-l the algorithm iden-ﬁes the best possible outcome, or un-l a stopping criterion is met. The resul-ng structure resembles a tree, with branches represen-ng decisions and leaves represen-ng the ﬁnal outcome or predic-on.  Mathema)cal Intui)on  The decision tree algorithm uses various metrics, such as Gini impurity or informa-on gain, to determine the op-mal feature for par--oning the data at each node. It selects the feature that maximizes the homogeneity of the target variable within each subset. The algorithm recursively applies this process to create a tree that best classiﬁes or predicts the target variable.  Limita)ons  Despite their versa-lity, decision trees can be prone to overﬁEng, especially when dealing with complex datasets. They may create overly complex trees', metadata={'page': 0, 'source': 'data/machine_learning_Decision Tree.pdf'})]"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["docs_mmr"]},{"cell_type":"code","execution_count":54,"id":"93b20226","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"93b20226","outputId":"1de8111a-0405-49c1-8038-84469ecf8795","tags":[]},"outputs":[{"data":{"text/plain":["'including its reliance on the linearity assump5on between the dependent and independent variables. I'"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["docs_mmr[0].page_content[:100]"]},{"cell_type":"code","execution_count":55,"id":"17d39762","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"17d39762","outputId":"bd00f0ae-7c3c-4c94-ce0e-8bef7a9e23cb","tags":[]},"outputs":[{"data":{"text/plain":["'their versa-lity, decision trees can be prone to overﬁEng, especially when dealing with complex data'"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["docs_mmr[1].page_content[:100]"]},{"cell_type":"markdown","id":"b2b909bc","metadata":{"id":"b2b909bc"},"source":["### Addressing Specificity: working with metadata\n","\n","In last lecture, we showed that a question about the third lecture can include results from other lectures as well.\n","\n","To address this, many vectorstores support operations on `metadata`.\n","\n","`metadata` provides context for each embedded chunk."]},{"cell_type":"code","execution_count":133,"id":"3c1a60b2","metadata":{"id":"3c1a60b2","tags":[]},"outputs":[],"source":["question = \"What are the advantages of Linear Regression ?\""]},{"cell_type":"code","execution_count":134,"id":"a8612840","metadata":{"id":"a8612840","tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["Number of requested results 20 is greater than number of elements in index 12, updating n_results = 12\n"]}],"source":["docs = vectordb.max_marginal_relevance_search(\n","    question,\n","    k=3,\n","    filter={\"source\":\"data/machine_learning_linear_reg.pdf\"}\n",")"]},{"cell_type":"code","execution_count":135,"id":"97031876","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"97031876","outputId":"f0b2162d-f65e-4e7b-9630-5a33acdca132","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["{'page': 0, 'source': 'data/machine_learning_linear_reg.pdf'}\n","{'page': 0, 'source': 'data/machine_learning_linear_reg.pdf'}\n","{'page': 1, 'source': 'data/machine_learning_linear_reg.pdf'}\n"]}],"source":["for d in docs:\n","    print(d.metadata)"]},{"cell_type":"code","execution_count":136,"id":"d4153ddb","metadata":{},"outputs":[{"data":{"text/plain":["[Document(page_content='including its reliance on the linearity assump5on between the dependent and independent variables. If the rela5onship between the variables is non-linear, the model may not accurately represent the data. Addi5onally, linear regression is sensi5ve to outliers, and its performance may be impacted by the presence of mul5collinearity among the independent variables.', metadata={'page': 0, 'source': 'data/machine_learning_linear_reg.pdf'}),\n"," Document(page_content='Linear Regression  Introduc)on  Linear regression is a fundamental supervised learning algorithm used in the ﬁeld of sta5s5cs and machine learning. It is employed to establish the rela5onship between a dependent variable and one or more independent variables. The objec5ve of linear regression is to ﬁnd the best-ﬁ?ng straight line that can depict the rela5onship between the variables. This line serves as a predic5ve model for future data points.  How It Works  Linear regression works by minimizing the ver5cal distances between the observed data points and the predicted values generated by the linear approxima5on. It accomplishes this through the method of least squares, which involves minimizing the sum of the squares of the diﬀerences between the observed and predicted values. The algorithm computes the slope and intercept of the line that minimizes the overall error, thereby determining the best-ﬁt line.  Mathema)cal Intui)on  The equa5on for a simple linear regression is represented as:   \\\\[Y = \\\\beta_0 + \\\\beta_1X + \\\\varepsilon\\\\]  where: - \\\\(Y\\\\) is the dependent variable, - \\\\(X\\\\) is the independent variable, - \\\\(\\\\beta_0\\\\) is the intercept, - \\\\(\\\\beta_1\\\\) is the coeﬃcient for the independent variable, - \\\\(\\\\varepsilon\\\\) represents the error term or residual.  Limita)ons  Linear regression has certain limita5ons, including its reliance on the linearity assump5on between the dependent and independent variables. If the rela5onship between the variables is', metadata={'page': 0, 'source': 'data/machine_learning_linear_reg.pdf'}),\n"," Document(page_content='Advantages  Despite its limita5ons, linear regression oﬀers various advantages. It provides a simple and interpretable framework for understanding the rela5onship between variables. It is computa5onally eﬃcient and well-suited for scenarios where the rela5onship between variables can be adequately captured by a linear model. Moreover, linear regression serves as a fundamental building block for more complex regression models and is widely used for predic5ve analy5cs and forecas5ng tasks.  Disadvantages  One of the signiﬁcant drawbacks of linear regression is its inability to capture complex rela5onships between variables. It may not perform well when the data exhibits non-linear paXerns. Addi5onally, it assumes that the errors follow a normal distribu5on, which might not hold true for all datasets. Moreover, the presence of outliers can signiﬁcantly impact the accuracy of the model, leading to biased results.  Understanding the intricacies and trade-oﬀs of linear regression is crucial for eﬀec5vely applying this technique in various real-world applica5ons.', metadata={'page': 1, 'source': 'data/machine_learning_linear_reg.pdf'})]"]},"execution_count":136,"metadata":{},"output_type":"execute_result"}],"source":["docs"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":5}
